{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from collections import OrderedDict\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-bandwidth",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-playing",
   "metadata": {},
   "source": [
    "Реализуйте базовый частотный метод по Шерлоку Холмсу:\n",
    "- подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "- возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), \n",
    "- зашифруйте их посредством случайной перестановки символов;\n",
    "- расшифруйте их таким частотным методом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alive-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# очистим текст от пунктуации и цифр\n",
    "def clean_text(\n",
    "    text, \n",
    "    alphabet=\"абвгдежзийклмнопрстуфхцчшщъыьэюя \"\n",
    "):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # only chars in alphabet\n",
    "    text = \"\".join([c for c in text if c in alphabet])\n",
    "    # remove double whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adult-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем частоту встречаемости нграмм\n",
    "def ngram_freq_dict(text, n_gram):\n",
    "    if n_gram > 1:\n",
    "        text = [\n",
    "            \"\".join(ngram) for ngram in ngrams(text, n=n_gram)\n",
    "        ]\n",
    "    freqs = {\n",
    "        k: v / len(text)\n",
    "        for k, v in sorted(Counter(text).items(), key=lambda item: item[1], reverse=True)\n",
    "        if v > 0\n",
    "    }\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "invalid-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем мэппинг для перестановки букв\n",
    "def create_mapping(freqs):\n",
    "    original_ngrams = list(freqs.keys())\n",
    "    permutated_ngrams = np.random.permutation(original_ngrams)\n",
    "    mapping = dict(zip(original_ngrams, permutated_ngrams))\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mexican-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# применяем мэппинг к тексту\n",
    "def apply_mapping(text, mapping):\n",
    "    return \"\".join([mapping.get(char, '*') for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cardiac-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_accuracy(text1, text2):\n",
    "    assert len(text1) == len(text2)\n",
    "    matching_chars = sum((char1 == char2) for char1, char2 in zip(text1, text2))\n",
    "    return matching_chars / len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beginning-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем тексты\n",
    "with open('data/AnnaKarenina.txt', 'r') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "with open('data/WarAndPeace.txt', 'r') as f:\n",
    "    corpus_text += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "purple-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# декодировщик\n",
    "def create_decode_mapping(corpus_freqs, encoded_text_freqs):\n",
    "    return dict(zip(encoded_text_freqs.keys(), corpus_freqs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intellectual-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# чистим текст\n",
    "corpus_text = clean_text(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "governing-smile",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# создаем частотную таблицу\n",
    "corpus_freqs = ngram_freq_dict(corpus_text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "breathing-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем кодировщик\n",
    "permutation_mapping = create_mapping(corpus_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tracked-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'ах постой ах сколько мыслей сколько надо спросить послушай ты ведь не можешь представить себе что ты сделал для меня тем что сказал я так счастлив что даже гадок стал я все забыл я нынче узнал что брат николай знаешь он тут я и про него забыл мне кажется что и он счастлив это вроде сумасшествия но одно ужасно вот ты женился ты знаешь это чувство ужасно то что мы старые уже с прошедшим не любви а грехов вдруг сближаемся с существом чистым невинным это отвратительно и поэтому нельзя не чувствовать себя недостойным'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "british-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кодируем текст\n",
    "encoded_text = apply_mapping(text, permutation_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "crude-conjunction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ьтфйпзыпгфьтфзяпдеяпфмаздхгфзяпдеяпфльипфзйнпзвыефйпздкшьгфыаф хиефлхфмпчхшефйнхизыь выефзхюхфщыпфыафзихдьдфидофмхлофыхмфщыпфзяьжьдфофыьяфзщьзыдв фщыпфиьчхфцьипяфзыьдфоф зхфжьюадфофлалщхфкжльдфщыпфюньыфлвяпдьгфжльхшефплфыкыфофвфйнпфлхцпфжьюадфмлхфяьчхызофщыпфвфплфзщьзыдв фъыпф нпихфзкмьзшхзы вофлпфпилпфкчьзлпф пыфыафчхлвдзофыафжльхшефъыпфщк зы пфкчьзлпфыпфщыпфмафзыьнахфкчхфзфйнпшхишвмфлхфдбю вфьфцнхтп ф инкцфзюдвчьхмзофзфзкрхзы пмфщвзыамфлх влламфъыпфпы ньывыхделпфвфйпъыпмкфлхдежофлхфщк зы п ьыефзхюофлхипзыпглам'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continued-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем частоты в закодированном тексте\n",
    "encoded_text_freqs = ngram_freq_dict(encoded_text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "steady-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем декодировщик\n",
    "decode_mapping = create_decode_mapping(corpus_freqs, encoded_text_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "speaking-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ию боаеой ию агосяго ркаснй агосяго тидо абьоавея боасмзий ек лндя тн роынзя бьндаеилвея анчн уео ек аднсис дсп рнтп енр уео агижис п еиг ауиаесвл уео диын шидог аеис п лан жичкс п тктун мжтис уео чьие твгосий жтинзя от еме п в бьо тншо жичкс ртн гиынеап уео в от ауиаесвл хео льодн амриазнаелвп то одто мыиато лое ек ынтвсап ек жтинзя хео умлаело мыиато ео уео рк аеиькн мын а бьозндзвр тн сэчлв и шьнюол лдьмш ачсвыинрап а амцнаелор уваекр тнлвтткр хео оельиевенсято в бохеорм тнсяжп тн умлаелолиея анчп тндоаеойткр'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# результат\n",
    "decoded_text = apply_mapping(encoded_text, decode_mapping)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "northern-manner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29593810444874274"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(text, decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-rover",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-aberdeen",
   "metadata": {},
   "source": [
    "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "- подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "- проведите тестирование аналогично п.1, но при помощи биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "liable-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_freqs = ngram_freq_dict(corpus_text, 2)\n",
    "encoded_text_freqs = ngram_freq_dict(encoded_text, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "middle-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decode_mapping_ngram(corpus_freqs, encoded_text_freqs, n_gram=2):\n",
    "    decode_mapping = {}\n",
    "    for encoded, corpus in zip(encoded_text_freqs.keys(), corpus_freqs.keys()):\n",
    "        for j in range(n_gram):\n",
    "            if encoded[j] not in decode_mapping.keys():\n",
    "                if corpus[j] not in decode_mapping.values():\n",
    "                    decode_mapping[encoded[j]] = corpus[j]\n",
    "    return decode_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fresh-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_mapping_ngrams = create_decode_mapping_ngram(corpus_freqs, encoded_text_freqs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "impressive-symbol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'кх иоеаоэ кх еьосяьо ылесйэ еьосяьо нкто еироедая иоесбвкэ ал зйтя нй ыогйвя ирйтеакздая ейчй пао ал етйскс тсж ыйнж айы пао еьк*кс ж акь епкеасдз пао ткгй *ктоь еакс ж зей *кчлс ж нлнпй б*нкс пао чрка ндьоскэ *нкйвя он аба ж д иро нй*о *кчлс ынй ькгйаеж пао д он епкеасдз уао зротй ебыкевйеаздж но отно бгкено зоа ал гйндсеж ал *нкйвя уао пбзеазо бгкено ао пао ыл еакрлй бгй е ировйтвды нй с*чзд к *рйхоз зтрб* ечсдгкйыеж е еб*йеазоы пдеалы нйздннлы уао оазркадайсяно д иоуаоыб нйся*ж нй пбзеазозкая ейчж нйтоеаоэнлы'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = apply_mapping(encoded_text, decode_mapping_ngrams)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "global-dealing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3404255319148936"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(text, decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-reggae",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-sussex",
   "metadata": {},
   "source": [
    "Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "- предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "- реализуйте и протестируйте его, убедитесь, что результаты улучшились.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "economic-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_freq_dict_smoothed(text, n_gram=2):\n",
    "    if n_gram > 1:\n",
    "        text = [\n",
    "            \"\".join(ngram) for ngram in ngrams(text, n=n_gram)\n",
    "        ]\n",
    "    freqs = {\n",
    "        k: (v + 1) / len(text)  # добавим 1 чтобы не было нулей\n",
    "        for k, v in Counter(text).items()\n",
    "    }\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "grave-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_log_proba(text, mapping, freqs, n_gram=2, alphabet=\"абвгдежзийклмнопрстуфхцчшщъыьэюя \"):\n",
    "    decoded_text = apply_mapping(text, mapping)\n",
    "    log_proba = 0.0\n",
    "    for i in range(len(decoded_text) - n_gram):\n",
    "        ngram = decoded_text[i : i + n_gram]\n",
    "        ngram_proba = freqs.get(\n",
    "            ngram, 1 / len(text)\n",
    "        )  # заменяем нули на 1 / len(text)\n",
    "        log_proba += np.log(ngram_proba)\n",
    "    return log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "saving-while",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "broadband-curve",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
