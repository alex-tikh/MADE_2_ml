{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-bandwidth",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-playing",
   "metadata": {},
   "source": [
    "Реализуйте базовый частотный метод по Шерлоку Холмсу:\n",
    "- подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "- возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), \n",
    "- зашифруйте их посредством случайной перестановки символов;\n",
    "- расшифруйте их таким частотным методом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alive-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# очистим текст от пунктуации и цифр\n",
    "def clean_text(\n",
    "    text, \n",
    "    alphabet=\"абвгдежзийклмнопрстуфхцчшщъыьэюя \"\n",
    "):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # only chars in alphabet\n",
    "    text = \"\".join([c for c in text if c in alphabet])\n",
    "    # remove double whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adult-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем частоту встречаемости нграмм\n",
    "def ngram_freq_dict(text, n_gram):\n",
    "    if n_gram > 1:\n",
    "        text = [\n",
    "            \"\".join(ngram) for ngram in ngrams(text, n=n_gram)\n",
    "        ]\n",
    "    freqs = {\n",
    "        k: v / len(text)\n",
    "        for k, v in sorted(Counter(text).items(), key=lambda item: item[1], reverse=True)\n",
    "        if v > 0\n",
    "    }\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "invalid-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем мэппинг для перестановки букв\n",
    "def create_mapping(freqs):\n",
    "    original_ngrams = list(freqs.keys())\n",
    "    permutated_ngrams = np.random.permutation(original_ngrams)\n",
    "    mapping = dict(zip(original_ngrams, permutated_ngrams))\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mexican-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# применяем мэппинг к тексту\n",
    "def apply_mapping(text, mapping):\n",
    "    return \"\".join([mapping.get(char, '*') for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cardiac-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_accuracy(text1, text2):\n",
    "    assert len(text1) == len(text2)\n",
    "    matching_chars = sum((char1 == char2) for char1, char2 in zip(text1, text2))\n",
    "    return matching_chars / len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beginning-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем тексты\n",
    "with open('data/AnnaKarenina.txt', 'r') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "with open('data/WarAndPeace.txt', 'r') as f:\n",
    "    corpus_text += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "purple-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# декодировщик\n",
    "def create_decode_mapping(corpus_freqs, encoded_text_freqs):\n",
    "    return dict(zip(encoded_text_freqs.keys(), corpus_freqs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intellectual-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# чистим текст\n",
    "corpus_text = clean_text(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "governing-smile",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# создаем частотную таблицу\n",
    "corpus_freqs = ngram_freq_dict(corpus_text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "breathing-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем кодировщик\n",
    "permutation_mapping = create_mapping(corpus_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tracked-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'ах постой ах сколько мыслей сколько надо спросить послушай ты ведь не можешь представить себе что ты сделал для меня тем что сказал я так счастлив что даже гадок стал я все забыл я нынче узнал что брат николай знаешь он тут я и про него забыл мне кажется что и он счастлив это вроде сумасшествия но одно ужасно вот ты женился ты знаешь это чувство ужасно то что мы старые уже с прошедшим не любви а грехов вдруг сближаемся с существом чистым невинным это отвратительно и поэтому нельзя не чувствовать себя недостойным'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "british-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кодируем текст\n",
    "encoded_text = apply_mapping(text, permutation_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "crude-conjunction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ьтфйпзыпгфьтфзяпдеяпфмаздхгфзяпдеяпфльипфзйнпзвыефйпздкшьгфыаф хиефлхфмпчхшефйнхизыь выефзхюхфщыпфыафзихдьдфидофмхлофыхмфщыпфзяьжьдфофыьяфзщьзыдв фщыпфиьчхфцьипяфзыьдфоф зхфжьюадфофлалщхфкжльдфщыпфюньыфлвяпдьгфжльхшефплфыкыфофвфйнпфлхцпфжьюадфмлхфяьчхызофщыпфвфплфзщьзыдв фъыпф нпихфзкмьзшхзы вофлпфпилпфкчьзлпф пыфыафчхлвдзофыафжльхшефъыпфщк зы пфкчьзлпфыпфщыпфмафзыьнахфкчхфзфйнпшхишвмфлхфдбю вфьфцнхтп ф инкцфзюдвчьхмзофзфзкрхзы пмфщвзыамфлх влламфъыпфпы ньывыхделпфвфйпъыпмкфлхдежофлхфщк зы п ьыефзхюофлхипзыпглам'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continued-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем частоты в закодированном тексте\n",
    "encoded_text_freqs = ngram_freq_dict(encoded_text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "steady-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем декодировщик\n",
    "decode_mapping = create_decode_mapping(corpus_freqs, encoded_text_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "speaking-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ию боаеой ию агосяго ркаснй агосяго тидо абьоавея боасмзий ек лндя тн роынзя бьндаеилвея анчн уео ек аднсис дсп рнтп енр уео агижис п еиг ауиаесвл уео диын шидог аеис п лан жичкс п тктун мжтис уео чьие твгосий жтинзя от еме п в бьо тншо жичкс ртн гиынеап уео в от ауиаесвл хео льодн амриазнаелвп то одто мыиато лое ек ынтвсап ек жтинзя хео умлаело мыиато ео уео рк аеиькн мын а бьозндзвр тн сэчлв и шьнюол лдьмш ачсвыинрап а амцнаелор уваекр тнлвтткр хео оельиевенсято в бохеорм тнсяжп тн умлаелолиея анчп тндоаеойткр'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# результат\n",
    "decoded_text = apply_mapping(encoded_text, decode_mapping)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "northern-manner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29593810444874274"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(text, decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-rover",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-aberdeen",
   "metadata": {},
   "source": [
    "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "- подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "- проведите тестирование аналогично п.1, но при помощи биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "liable-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_freqs = ngram_freq_dict(corpus_text, 2)\n",
    "encoded_text_freqs = ngram_freq_dict(encoded_text, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "middle-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decode_mapping_ngram(corpus_freqs, encoded_text_freqs, n_gram=2):\n",
    "    decode_mapping = {}\n",
    "    for encoded, corpus in zip(encoded_text_freqs.keys(), corpus_freqs.keys()):\n",
    "        for j in range(n_gram):\n",
    "            if encoded[j] not in decode_mapping.keys():\n",
    "                if corpus[j] not in decode_mapping.values():\n",
    "                    decode_mapping[encoded[j]] = corpus[j]\n",
    "    return decode_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fresh-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_mapping_ngrams = create_decode_mapping_ngram(corpus_freqs, encoded_text_freqs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "impressive-symbol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'кх иоеаоэ кх еьосяьо ылесйэ еьосяьо нкто еироедая иоесбвкэ ал зйтя нй ыогйвя ирйтеакздая ейчй пао ал етйскс тсж ыйнж айы пао еьк*кс ж акь епкеасдз пао ткгй *ктоь еакс ж зей *кчлс ж нлнпй б*нкс пао чрка ндьоскэ *нкйвя он аба ж д иро нй*о *кчлс ынй ькгйаеж пао д он епкеасдз уао зротй ебыкевйеаздж но отно бгкено зоа ал гйндсеж ал *нкйвя уао пбзеазо бгкено ао пао ыл еакрлй бгй е ировйтвды нй с*чзд к *рйхоз зтрб* ечсдгкйыеж е еб*йеазоы пдеалы нйздннлы уао оазркадайсяно д иоуаоыб нйся*ж нй пбзеазозкая ейчж нйтоеаоэнлы'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = apply_mapping(encoded_text, decode_mapping_ngrams)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "global-dealing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3404255319148936"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(text, decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-reggae",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-sussex",
   "metadata": {},
   "source": [
    "Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "- предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "- реализуйте и протестируйте его, убедитесь, что результаты улучшились.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-street",
   "metadata": {},
   "source": [
    "Текст, разбитый на биграммы - это марковская цепь, где вероятности переходов - это вероятности биграмм.\\\n",
    "Вероятность встретить имеено такую последовательность биграмм - произведение вероятностей биграмм.\n",
    "\n",
    "Пусть у нас есть закодированный текст и уже есть декодировщик. Применив декодировщик, мы получим какую-то последовательность и можем посчитать, какова вероятность ее встретить. Таким образом, перебрав все возможные декодировщики (а значит и все возможные последовательности), мы найдем самый вероятный декодировщик.\\\n",
    "Но мы не можем перебрать все возможные последовательности, здесть-то мы и применим MCMC-алгоритм!\n",
    "\n",
    "Так как алгоритм жадный, то будем применять его несколько раз, пока не получим лучшую вероятность\n",
    "\n",
    "- инициализируем тождественный декодировщик, посчитаем правдоподобие\n",
    "- меняем пару ключей в декодировщике\n",
    "- считаем новое правдоподобие\n",
    "- берем их отношение - это будет вероятность принять изменение в декодировщике\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "economic-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_freq_dict_smoothed(text, n_gram=2):\n",
    "    if n_gram > 1:\n",
    "        text = [\n",
    "            \"\".join(ngram) for ngram in ngrams(text, n=n_gram)\n",
    "        ]\n",
    "    freqs = {\n",
    "        k: (v + 1) / len(text)  # добавим 1 чтобы не было нулей\n",
    "        for k, v in Counter(text).items()\n",
    "    }\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "grave-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_log_proba(text, mapping, freqs, n_gram=2, alphabet=\"абвгдежзийклмнопрстуфхцчшщъыьэюя \"):\n",
    "    decoded_text = apply_mapping(text, mapping)\n",
    "    log_proba = 0\n",
    "    for i in range(len(decoded_text) - n_gram):\n",
    "        ngram = decoded_text[i : i + n_gram]\n",
    "        ngram_proba = freqs.get(ngram, 1 / len(text))  # заменяем нули на 1 / len(text)\n",
    "        log_proba += np.log(ngram_proba)\n",
    "    return log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "saving-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decode_mapping_mcmc(\n",
    "    encoded_text,\n",
    "    alphabet_encoded,\n",
    "    alphabet_corpus,\n",
    "    corpus_freqs,\n",
    "    n_iters=10000,\n",
    "    n_trials=10,\n",
    "    n_gram=2,\n",
    "):\n",
    "    \n",
    "    best_decode_mapping = None\n",
    "    best_log_likelihood = -np.inf\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        alphabet_encoded = list(alphabet_encoded)\n",
    "        alphabet_trial = list(alphabet_corpus)\n",
    "        decode_mapping = dict(zip(alphabet_encoded, alphabet_trial))\n",
    "        \n",
    "        log_proba_current = text_log_proba(\n",
    "            encoded_text, decode_mapping, corpus_freqs, n_gram=n_gram\n",
    "        )\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            alphabet_iter = alphabet_trial.copy()\n",
    "            \n",
    "            idx1, idx2 = random.sample(range(len(alphabet_iter)), 2)\n",
    "            alphabet_iter[idx1], alphabet_iter[idx2] = alphabet_iter[idx2], alphabet_iter[idx1]\n",
    "            \n",
    "            decode_mapping_iter = dict(zip(alphabet_encoded, alphabet_iter))\n",
    "            \n",
    "            log_proba_iter = text_log_proba(\n",
    "                encoded_text, decode_mapping_iter, corpus_freqs, n_gram=n_gram\n",
    "            )\n",
    "\n",
    "            p_accept = np.exp(log_proba_iter - log_proba_current)\n",
    "\n",
    "            if p_accept > random.uniform(0,1):\n",
    "                alphabet_trial = alphabet_iter\n",
    "                log_proba_current = log_proba_iter\n",
    "                decode_mapping = decode_mapping_iter\n",
    "\n",
    "        if log_proba_current > best_log_likelihood:\n",
    "            best_log_likelihood = log_proba_current\n",
    "            best_decode_mapping = decode_mapping\n",
    "\n",
    "\n",
    "    print(f\"Best likelihood: {best_log_likelihood}\")\n",
    "    return best_decode_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "detailed-commander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -2809.027342296167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ах постой ах сколько мыслей сколько надо спросить послуъай ты ведь не можеъь представить себе что ты сделал для меня тем что сказал я так счастлив что даже гадок стал я все забыл я нынче узнал что брат николай знаеъь он тут я и про него забыл мне кажется что и он счастлив это вроде сумасъествия но одно ужасно вот ты женился ты знаеъь это чувство ужасно то что мы старые уже с проъедъим не любви а грехов вдруг сближаемся с сушеством чистым невинным это отвратительно и поэтому нельзя не чувствовать себя недостойным'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_freqs = ngram_freq_dict_smoothed(corpus_text, n_gram=2)\n",
    "\n",
    "decode_mapping_mcmc = create_decode_mapping_mcmc(\n",
    "    encoded_text,\n",
    "    alphabet_encoded=\"абвгдежзийклмнопрстуфхцчшщъыьэюя \",\n",
    "    alphabet_corpus=\"абвгдежзийклмнопрстуфхцчшщъыьэюя \",\n",
    "    corpus_freqs=corpus_freqs,\n",
    ")\n",
    "\n",
    "decoded_text = apply_mapping(encoded_text, decode_mapping_mcmc)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "finished-atlantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9845261121856866"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(text, decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-level",
   "metadata": {},
   "source": [
    "## Задание 4\n",
    "Расшифруйте сообщение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "natural-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "occasional-headline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'↹←⇛↟⇒↝⇴↨⇠⇯↷⇌⇊⇞⇈⇷↤↳↾↙⇙↲↞⇆⇰⇸↘⇏'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(ngram_freq_dict(message, 1).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "still-valve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'↤⇯↟⇷⇏↲⇈⇛⇰↙⇠⇸↹↝↨⇞↾⇆←⇴↳↘⇙↷⇒⇊↞⇌'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(set(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "shaped-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_corpus = \"\".join(ngram_freq_dict(corpus_text, 1).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "unusual-customs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' оеанитслвркдмупяьгыбзчжйшхюэцщфъ'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "suspended-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_message = \"\".join(ngram_freq_dict(message, 1).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "meaning-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -1208.156892501107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'если вй вимите ноъзальнйы или порти ноъзальнйы текст у жтого соочдения котоъйы легко пъоритать скоъее всего вй все смелали пъавильно и полурите заксизальнйы чалл эа послемнее ретвеътое эамание куъса ботя конерно я нирего не очедаш'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_mapping_mcmc = create_decode_mapping_mcmc(\n",
    "    message,\n",
    "    alphabet_encoded=alphabet_message,\n",
    "    alphabet_corpus=alphabet_corpus,\n",
    "    corpus_freqs=corpus_freqs,\n",
    "    n_iters=10000,\n",
    "    n_trials=100\n",
    ")\n",
    "\n",
    "decoded_text = apply_mapping(message, decode_mapping_mcmc)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-class",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
