#  Advanced machine learning (MADE S02E02)
This repository contains notebooks for the Advanced machine learning course.

**Tip #1:**

Loading the entire repository can take a considerable amount of time. A single folder can be downloaded via [DownGit](https://downgit.github.io/).

**Tip #2:**

Sometimes GitHub failes to render a notebook. In that case use [nbviewer](https://nbviewer.jupyter.org/) — it works like a charm!

**Tip #3:**
All materials are available on [the course page](https://logic.pdmi.ras.ru/~sergey/teaching/mademl2021.html)

### Lectures

Legend: ![](./icons/pdf.png) — slides, ![](./icons/jupyter.png) — code, ![](./icons/youtube.png) — video.

Week | What | Where | When
:--: | :--: | :---: | :--:
[1](https://data.mail.ru/curriculum/program/lesson/16323/) | Введение. История AI. Что такое машинное обучение, какие задачи оно решает. Байесовский подход в машинном обучении, теорема Байеса и её интерпретация. Байесовский анализ испытаний Бернулли. | [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_1/01-intro-board.pdf) [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_1/01-intro.pdf) [![](./icons/jupyter.png)](https://nbviewer.jupyter.org/github.com/alex-tikh/MADE_2_ml/blob/master/lecture_1/01-intro.ipynb) [![](./icons/youtube.png)](https://youtu.be/H3LVXu7tSmg) | 23.01.2021
[2](https://data.mail.ru/curriculum/program/lesson/16324/) | Введение в байесовский вывод: полный анализ бросаний монетки, теорема Байеса в жизни, hot hand fallacy. Линейная регрессия: метод наименьших квадратов. | [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_2/02-bayes-board.pdf) [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_2/02-bayes.pdf) [![](./icons/jupyter.png)](https://nbviewer.jupyter.org/github.com/alex-tikh/MADE_2_ml/blob/master/lecture_2/02-bayes.ipynb) [![](./icons/youtube.png)](https://youtu.be/HF8MlOPvB5Y) | 30.01.2021
[3](https://data.mail.ru/curriculum/program/lesson/16325/) | Линейная регрессия: вероятностный смысл, априорные распределения, оверфиттинг и регуляризация, предсказания, эквивалентное ядро. | [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_3/03-linregr-board.pdf) [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_3/03-linregr.pdf) [![](./icons/jupyter.png)](https://nbviewer.jupyter.org/github.com/alex-tikh/MADE_2_ml/blob/master/lecture_3/03-linregr.ipynb) [![](./icons/youtube.png)](https://youtu.be/mCczRVUuQoQ) | 06.02.2021
[4](https://data.mail.ru/curriculum/program/lesson/16326/) | Несколько важных сюжетов: байесовский выбор моделей, проклятие размерности, разложение bias-variance-noise. | [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_4/04-statdecision-board.pdf) [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_4/04-statdecision.pdf) [![](./icons/jupyter.png)](https://nbviewer.jupyter.org/github.com/alex-tikh/MADE_2_ml/blob/master/lecture_4/04-statdecision.ipynb) [![](./icons/youtube.png)](https://youtu.be/PtVXHjkozbs) | 13.02.2021
[5](https://data.mail.ru/curriculum/program/lesson/16327/) | Задачи классификации: геометрический смысл, порождающие модели и LDA/QDA, логистическая регрессия. | [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_5/05-classification-board.pdf) [![](./icons/pdf.png)](https://github.com/alex-tikh/MADE_2_ml/blob/master/lecture_5/05-classification.pdf) [![](./icons/jupyter.png)](https://nbviewer.jupyter.org/github.com/alex-tikh/MADE_2_ml/blob/master/lecture_5/05-classification.ipynb) [![](./icons/youtube.png)](https://youtu.be/yi_pstsjvAs) | 20.02.2021
